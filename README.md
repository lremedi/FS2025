# What motivated this way of presenting after my Udemy course:

(didn't even know how to title this)

I wanted to do things, but I didn't know where to start.

[Hugging Face](https://huggingface.co/):

For example:
https://huggingface.co/learn/cookbook/en/code_search

![image](https://github.com/user-attachments/assets/1f071b79-2fe5-4d94-8c0e-92bba7f412f4)

## What's a Model?

A model is the mathematical framework or algorithm used to make predictions or perform specific tasks, such as recognizing images or generating text.

- Types of Models: Linear regression, decision trees, neural networks, transformers, etc.
- Applications: Language models like GPT, vision models like ResNet, and multi-modal models like CLIP.
- Key Features: Models are trained on data and rely on optimization techniques to minimize error during prediction.

## What's a Tokenizer?

A tokenizer is a preprocessing tool that breaks text data into manageable units (tokens) for model processing.

### Types of Tokenization:
 - Word-based: Splits text by spaces.
 - Subword-based: Splits words into smaller meaningful chunks (e.g., Byte Pair Encoding).
 - Character-based: Breaks text into individual characters.
 - Purpose: Converts text into numerical formats that models can process efficiently.

## What's the Model Tokenizer?

A model tokenizer is a critical component in Natural Language Processing (NLP) pipelines that prepares raw text data for use by a machine learning model. It is closely tied to the specific model it supports, ensuring that the text is broken down into a format the model can understand and process effectively.

### Why is a Tokenizer Model-Specific?
1. Vocabulary Alignment:
Each model is trained with a specific vocabulary (a set of all possible tokens). The tokenizer ensures that the text is split and encoded in a way that aligns with the model's training.

2. Tokenization Strategy:
Different models use different tokenization methods depending on their architecture. For example:
 - BERT: Uses WordPiece Tokenization, breaking words into subwords if theyâ€™re not in the vocabulary.
 - GPT: Uses Byte-Pair Encoding (BPE) to merge frequent character pairs into tokens.
 - T5: Uses a SentencePiece tokenizer for subword segmentation.
3. Special Tokens:
Models often require special tokens like [CLS] (classification), [SEP] (separator), or [PAD] (padding).
The tokenizer ensures these tokens are correctly added to the input.
    ```python
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    text = "I love programming with Python!"
    tokens = tokenizer(text, padding="max_length", max_length=10, truncation=True, return_tensors="pt")
    print("Tokens:", tokenizer.convert_ids_to_tokens(tokens["input_ids"][0]))
    ```
    ```lua
    Tokens: ['[CLS]', 'i', 'love', 'programming', 'with', 'python', '!', '[SEP]', '[PAD]', '[PAD]']
    ```
    [CLS] and [SEP] are special tokens.
Tokens not in the vocabulary (e.g., "programming") may remain intact or split into smaller subwords.


## What are embeddings?
Embeddings are numerical vector representations of words, phrases, or data points that capture their semantic meaning.

1. Key Characteristics:
  - Words with similar meanings are closer in the embedding space.
  - Generated by neural networks like Word2Vec, GloVe, or transformers.
    
2. Applications:
  - Sentence similarity comparisons.
  - Input for downstream tasks like classification or clustering.

## What's model embedding?
Each model has its own way of creating embeddings based on its architecture, training data, and tokenization strategy.

Model-specific embedding creation refers to the process of generating vector representations (embeddings) of text using a model that has been specifically trained to understand and map that text into a dense vector space. Each model has its own way of creating embeddings based on its architecture, training data, and tokenization strategy.

The embeddings generated by a pretrained model encode a broad understanding of language, which can then be used for specific tasks or fine-tuned for specialized use cases.
By choosing the right model, you can ensure that the embeddings generated are well-suited for your task, whether it's text classification, semantic search, or something else entirely.

### Why Pretraining Matters for Embeddings
 - Semantic Understanding: Pretraining allows the model to understand language structures and capture nuanced meanings. For example, synonyms like "happy" and "joyful" will have similar embeddings because of their context in training data.
 - Generalization: Pretrained models can handle unseen text effectively by relying on patterns learned during training.
 - Task-Specific Optimization: Models like MiniLM are fine-tuned on tasks like semantic similarity or clustering, making their embeddings especially suitable for such use cases.

## Transformers
A transformer is a neural network architecture that processes sequential data efficiently using self-attention.

## Attention Mechanism
Attention is a mechanism that allows models to focus on the most relevant parts of the input data dynamically.
Types:

- Self-Attention: Compares all tokens in the input sequence to each other.
- Cross-Attention: Maps input tokens to another sequence (e.g., in encoder-decoder setups).
- Role in Transformers: Revolutionized sequence processing by overcoming the limitations of older architectures like RNNs.

## What is Cosine Similarity?
**Cosine similarity** measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It evaluates how aligned or similar two vectors are, regardless of their magnitude.

- **Range**:
  - **1**: Vectors point in the same direction (high similarity).
  - **0**: Vectors are orthogonal (no similarity).
  - **-1**: Vectors point in opposite directions (high dissimilarity).

### **Mathematical Formula**
For two vectors \(A\) and \(B\):
![image](https://github.com/user-attachments/assets/6dd8091d-2eb6-4b38-b41e-be8aa0ca3d9c)

Where:
- \(A \cdot B\): Dot product of the two vectors.
- \(||A||\): Magnitude of vector \(A\).
- \(||B||\): Magnitude of vector \(B\).

---

## **2. Why Use Cosine Similarity?**
- **Magnitude Independence**: Focuses on vector direction, ignoring differences in magnitude.
- **High-Dimensional Data**: Effective for sparse, high-dimensional representations like text embeddings.

---

## **3. Applications of Cosine Similarity**
- **Text Similarity**: Compare sentences or documents.
- **Semantic Search**: Retrieve documents most similar to a query.
- **Clustering**: Group similar embeddings in tasks like topic modeling.
- **Recommendation Systems**: Identify items similar to a given item.
